{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e493c47a",
   "metadata": {},
   "source": [
    "1. 数组中第K大的元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5e5228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "\n",
    "nums = [1,2,3,4,5,6,7,8,9]\n",
    "k = 3\n",
    "\n",
    "min_heap = []\n",
    "for i, num in enumerate(nums):\n",
    "    if i < k:\n",
    "        heapq.heappush(min_heap, num)\n",
    "    else:\n",
    "        if num > min_heap[0]:\n",
    "            heapq.heappop(min_heap)\n",
    "            heapq.heappush(min_heap, num)\n",
    "print(min_heap[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0666125e",
   "metadata": {},
   "source": [
    "2. 最长不重复子串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "724e9278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "str = \"abcabcbb\"\n",
    "left = 0 \n",
    "last_occur = {}\n",
    "max_len = 0\n",
    "for right, char in enumerate(str):\n",
    "    if char in last_occur and last_occur[char] >= left:\n",
    "        left = last_occur[char] + 1\n",
    "    last_occur[char] = right\n",
    "    str_len = right - left + 1\n",
    "    if str_len > max_len:\n",
    "        max_len = str_len\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d205104d",
   "metadata": {},
   "source": [
    "3. 实现Multi-head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c755a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_size // num_heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * num_heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by num_heads\"\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(num_heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        # Split the embedding into `num_heads` different pieces\n",
    "        values = values.reshape(N, value_len, self.num_heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.num_heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.num_heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)  # (N, value_len, num_heads, head_dim)\n",
    "        keys = self.keys(keys)  # (N, key_len, num_heads, head_dim)\n",
    "        queries = self.queries(queries)  # (N, query_len, num_heads, head_dim)\n",
    "\n",
    "        # Einsum does batched matrix multiplication for query*keys for each training example\n",
    "        # with shape (N, num_heads, query_len, key_len)\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.num_heads * self.head_dim\n",
    "        )\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cd016e",
   "metadata": {},
   "source": [
    "4. 多面骰子n次和大于等于k的概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a54eaf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "def calculate_probobility(m, target):\n",
    "    dp = [[0] * (m * 6 + 1) for i in range(m)]\n",
    "    for i in range(1, 7):\n",
    "        dp[0][i] = 1/6\n",
    "    for i in range(1, m):\n",
    "        for j in range(1, i * 6 + 1):\n",
    "            for k in range(1, 7):\n",
    "                dp[i][j + k] += dp[i-1][j] / 6\n",
    "    return sum(dp[m-1][target:])\n",
    "\n",
    "print(calculate_probobility(2, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3f1003",
   "metadata": {},
   "source": [
    "5. 字母金字塔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff6343d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    A    \n",
      "   ABA   \n",
      "  ABCBA  \n",
      " ABCDCBA \n",
      "ABCDEDCBA\n"
     ]
    }
   ],
   "source": [
    "def left_padding(s, n):\n",
    "    return ' ' * (n - len(s)) + s\n",
    "\n",
    "def reverse(s):\n",
    "    return s[::-1]\n",
    "\n",
    "def alphabeta_pyramid(s):\n",
    "    n = len(s)\n",
    "    for i in range(n):\n",
    "        s_ = left_padding(s[:i+1], n)\n",
    "        s_ = s_ + reverse(s_[:-1])\n",
    "        print(s_)\n",
    "\n",
    "alphabeta_pyramid(\"ABCDE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec4c086",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyroflux",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
